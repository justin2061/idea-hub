#!/usr/bin/env python3
"""
AI å…§å®¹å¡«å……å·¥å…·
ä½¿ç”¨ Claude API è‡ªå‹•å¡«å……æ–‡ç« ä¸­çš„ TODO éƒ¨åˆ†

ä½¿ç”¨æ–¹æ³•ï¼š
python ai_content_filler.py --file path/to/article.md --api-key YOUR_API_KEY
"""

import os
import re
import argparse
import anthropic
from pathlib import Path
import json
import hashlib
from functools import lru_cache

class AIContentFiller:
    def __init__(self, api_key=None, use_cache=True):
        self.api_key = api_key or os.environ.get('ANTHROPIC_API_KEY')
        if self.api_key:
            self.client = anthropic.Anthropic(api_key=self.api_key)
        else:
            self.client = None
            print("âš ï¸  æœªè¨­å®š API Keyï¼Œå°‡ä½¿ç”¨äº’å‹•æ¨¡å¼")

        # æ€§èƒ½å„ªåŒ–ï¼šæ·»åŠ ç·©å­˜æ”¯æŒ
        self.use_cache = use_cache
        self.cache_dir = Path('_tests/.cache')
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / 'ai_content_cache.json'
        self.cache = self._load_cache()

    def _load_cache(self):
        """è¼‰å…¥ç·©å­˜ï¼ˆæ€§èƒ½å„ªåŒ–ï¼‰"""
        if self.use_cache and self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}

    def _save_cache(self):
        """ä¿å­˜ç·©å­˜ï¼ˆæ€§èƒ½å„ªåŒ–ï¼‰"""
        if self.use_cache:
            try:
                with open(self.cache_file, 'w', encoding='utf-8') as f:
                    json.dump(self.cache, f, indent=2, ensure_ascii=False)
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜ç·©å­˜å¤±æ•—: {e}")

    def _get_cache_key(self, prompt: str) -> str:
        """ç”Ÿæˆç·©å­˜éµï¼ˆæ€§èƒ½å„ªåŒ–ï¼‰"""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()

    def extract_todos(self, content):
        """æå–æ–‡ç« ä¸­çš„æ‰€æœ‰ TODO é …ç›®"""
        # åŒ¹é… [ ] TODO: ... æ ¼å¼
        pattern = r'\[ \] TODO:?\s*(.+?)(?:\n|$)'
        todos = re.findall(pattern, content)
        return todos

    def extract_context(self, content, todo_position):
        """æå– TODO å‘¨åœçš„ä¸Šä¸‹æ–‡"""
        lines = content.split('\n')
        todo_line = -1

        for i, line in enumerate(lines):
            if '[ ] TODO' in line and todo_position in line:
                todo_line = i
                break

        if todo_line == -1:
            return ""

        # æå–å‰å¾Œ 5 è¡Œä½œç‚ºä¸Šä¸‹æ–‡
        start = max(0, todo_line - 5)
        end = min(len(lines), todo_line + 5)
        context_lines = lines[start:end]

        return '\n'.join(context_lines)

    def generate_content_with_ai(self, todo_item, context, article_title):
        """ä½¿ç”¨ AI ç”Ÿæˆå…§å®¹ï¼ˆå¸¶ç·©å­˜å„ªåŒ–ï¼‰"""
        if not self.client:
            return self.interactive_mode(todo_item, context)

        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡æŠ€è¡“å¯«ä½œåŠ©æ‰‹ã€‚

æ–‡ç« æ¨™é¡Œï¼š{article_title}

ç•¶å‰éœ€è¦å¡«å¯«çš„éƒ¨åˆ†ï¼š
{todo_item}

ä¸Šä¸‹æ–‡ï¼š
{context}

è«‹æ ¹æ“šä¸Šä¸‹æ–‡ï¼Œç”¨ç¹é«”ä¸­æ–‡æ’°å¯«é€™å€‹éƒ¨åˆ†çš„å…§å®¹ã€‚è¦æ±‚ï¼š
1. å…§å®¹æ·±å…¥ä¸”å¯¦ç”¨
2. ä¿æŒå°ˆæ¥­ä½†æ˜“æ‡‚çš„èªèª¿
3. ä½¿ç”¨å…·é«”ä¾‹å­å’Œæ•¸æ“š
4. æ ¼å¼æ¸…æ™°ï¼Œé©åˆéƒ¨è½æ ¼é–±è®€
5. é•·åº¦é©ä¸­ï¼ˆ100-300å­—ï¼‰

åªè¼¸å‡ºå…§å®¹æœ¬èº«ï¼Œä¸è¦åŒ…å«å…¶ä»–èªªæ˜ã€‚
"""

        # æ€§èƒ½å„ªåŒ–ï¼šæª¢æŸ¥ç·©å­˜
        cache_key = self._get_cache_key(prompt)
        if self.use_cache and cache_key in self.cache:
            print("  âš¡ ä½¿ç”¨ç·©å­˜å…§å®¹")
            return self.cache[cache_key]

        try:
            message = self.client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=1000,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            content = message.content[0].text

            # ä¿å­˜åˆ°ç·©å­˜
            if self.use_cache:
                self.cache[cache_key] = content
                self._save_cache()

            return content
        except Exception as e:
            print(f"âŒ AI ç”Ÿæˆå¤±æ•—: {e}")
            return self.interactive_mode(todo_item, context)

    def interactive_mode(self, todo_item, context):
        """äº’å‹•æ¨¡å¼ï¼šæ‰‹å‹•è¼¸å…¥å…§å®¹"""
        print(f"\nğŸ“ éœ€è¦å¡«å¯«ï¼š{todo_item}")
        print(f"ä¸Šä¸‹æ–‡ï¼š\n{context}\n")
        print("è«‹è¼¸å…¥å…§å®¹ï¼ˆè¼¸å…¥ 'skip' è·³éï¼‰ï¼š")

        lines = []
        while True:
            line = input()
            if line.lower() == 'skip':
                return None
            if line.lower() == 'done' or line == '':
                break
            lines.append(line)

        return '\n'.join(lines)

    def fill_article(self, filepath, auto_mode=False):
        """å¡«å……æ–‡ç« ä¸­çš„ TODO é …ç›®"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–æ¨™é¡Œ
        title_match = re.search(r'title:\s*"(.+?)"', content)
        article_title = title_match.group(1) if title_match else "æœªçŸ¥æ¨™é¡Œ"

        print(f"ğŸ“„ è™•ç†æ–‡ç« ï¼š{article_title}")
        print(f"ğŸ“Š ç™¼ç¾ {len(self.extract_todos(content))} å€‹ TODO é …ç›®\n")

        # é€å€‹è™•ç† TODO
        todo_pattern = r'(\[ \] TODO:?\s*)(.+?)(?=\n|$)'

        def replace_todo(match):
            todo_marker = match.group(1)
            todo_content = match.group(2)

            print(f"\nğŸ” è™•ç†ï¼š{todo_content[:50]}...")

            if auto_mode and self.client:
                # è‡ªå‹•æ¨¡å¼ï¼šä½¿ç”¨ AI ç”Ÿæˆ
                context = self.extract_context(content, todo_content)
                generated = self.generate_content_with_ai(todo_content, context, article_title)

                if generated:
                    print(f"âœ… å·²ç”Ÿæˆå…§å®¹")
                    return generated
                else:
                    return match.group(0)  # ä¿æŒåŸæ¨£
            else:
                # äº’å‹•æ¨¡å¼ï¼šè©¢å•ç”¨æˆ¶
                response = input(f"è¦å¡«å……é€™å€‹ TODO å—ï¼Ÿ(y/n/auto): ").lower()

                if response == 'y':
                    context = self.extract_context(content, todo_content)
                    generated = self.generate_content_with_ai(todo_content, context, article_title)
                    return generated if generated else match.group(0)
                elif response == 'auto' and self.client:
                    context = self.extract_context(content, todo_content)
                    generated = self.generate_content_with_ai(todo_content, context, article_title)
                    return generated if generated else match.group(0)
                else:
                    return match.group(0)  # ä¿æŒåŸæ¨£

        # æ›¿æ›æ‰€æœ‰ TODO
        new_content = re.sub(todo_pattern, replace_todo, content)

        # ä¿å­˜çµæœ
        output_path = filepath.replace('.md', '_filled.md')
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(new_content)

        print(f"\nâœ… è™•ç†å®Œæˆï¼æ–°æ–‡ä»¶ï¼š{output_path}")
        return output_path

    def analyze_article(self, filepath):
        """åˆ†ææ–‡ç« çš„å®Œæˆåº¦"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()

        todos = self.extract_todos(content)
        total_lines = len(content.split('\n'))
        word_count = len(content)

        # æå–æ¨™é¡Œ
        title_match = re.search(r'title:\s*"(.+?)"', content)
        title = title_match.group(1) if title_match else "æœªçŸ¥"

        print(f"\nğŸ“Š æ–‡ç« åˆ†æå ±å‘Š")
        print(f"=" * 50)
        print(f"æ¨™é¡Œï¼š{title}")
        print(f"ç¸½è¡Œæ•¸ï¼š{total_lines}")
        print(f"ç¸½å­—æ•¸ï¼š{word_count}")
        print(f"å¾…å®Œæˆé …ç›®ï¼š{len(todos)}")
        print(f"å®Œæˆåº¦ï¼š{((total_lines - len(todos)) / total_lines * 100):.1f}%")
        print(f"\nå¾…å®Œæˆé …ç›®ï¼š")
        for i, todo in enumerate(todos[:10], 1):  # åªé¡¯ç¤ºå‰ 10 å€‹
            print(f"{i}. {todo[:60]}...")

        if len(todos) > 10:
            print(f"... é‚„æœ‰ {len(todos) - 10} å€‹é …ç›®")

def main():
    parser = argparse.ArgumentParser(description='AI å…§å®¹å¡«å……å·¥å…·')
    parser.add_argument('--file', '-f', required=True, help='æ–‡ç« æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--api-key', '-k', help='Anthropic API Key')
    parser.add_argument('--auto', '-a', action='store_true', help='è‡ªå‹•æ¨¡å¼ï¼ˆä¸è©¢å•ï¼‰')
    parser.add_argument('--analyze', action='store_true', help='åªåˆ†æä¸å¡«å……')

    args = parser.parse_args()

    # å‰µå»ºå¡«å……å™¨
    filler = AIContentFiller(api_key=args.api_key)

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.file):
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨ï¼š{args.file}")
        return

    if args.analyze:
        # åªåˆ†æ
        filler.analyze_article(args.file)
    else:
        # å¡«å……å…§å®¹
        filler.fill_article(args.file, auto_mode=args.auto)

if __name__ == '__main__':
    main()
