#!/usr/bin/env python3
"""
è‡ªå‹•å…§å®¹ç”Ÿæˆå™¨ - ä¸»ç¨‹å¼
æ ¹æ“šåˆ†é¡è‡ªå‹•æœå°‹ç†±é–€äº‹ä»¶ï¼Œä½¿ç”¨ AI ç”Ÿæˆæ–‡ç« ï¼Œä¸¦è‡ªå‹•ç™¼å¸ƒ

ä½¿ç”¨æ–¹æ³•ï¼š
python auto_content_generator.py --categories all --count 5 --publish
"""

import os
import sys
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import anthropic
import requests
from typing import List, Dict

class AutoContentGenerator:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.environ.get('ANTHROPIC_API_KEY')
        self.client = anthropic.Anthropic(api_key=self.api_key) if self.api_key else None

        # æ–‡ç« åˆ†é¡é…ç½®
        self.categories = {
            'ai-tools': {
                'name': 'AIå·¥å…·',
                'keywords': ['AI', 'ChatGPT', 'Claude', 'äººå·¥æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'AIå·¥å…·'],
                'tags': ['AI', 'AIå·¥å…·', 'äººå·¥æ™ºæ…§'],
                'search_queries': [
                    'AI tools 2024',
                    'ChatGPT updates',
                    'Claude AI',
                    'Gemini AI',
                    'new AI applications'
                ]
            },
            'creativity': {
                'name': 'å‰µæ„æ€ç¶­',
                'keywords': ['å‰µæ„', 'è¨­è¨ˆæ€ç¶­', 'å‰µæ–°', 'è…¦åŠ›æ¿€ç›ª', 'SCAMPER'],
                'tags': ['å‰µæ„', 'å‰µæ–°', 'è¨­è¨ˆæ€ç¶­'],
                'search_queries': [
                    'design thinking 2024',
                    'creative innovation',
                    'brainstorming techniques',
                    'innovation methods'
                ]
            },
            'productivity': {
                'name': 'ç”Ÿç”¢åŠ›',
                'keywords': ['ç”Ÿç”¢åŠ›', 'æ•ˆç‡', 'æ™‚é–“ç®¡ç†', 'GTD', 'å°ˆæ³¨'],
                'tags': ['ç”Ÿç”¢åŠ›', 'æ•ˆç‡', 'æ™‚é–“ç®¡ç†'],
                'search_queries': [
                    'productivity tools 2024',
                    'time management techniques',
                    'workflow optimization',
                    'productivity hacks'
                ]
            },
            'entrepreneurship': {
                'name': 'å‰µæ¥­',
                'keywords': ['å‰µæ¥­', 'startup', 'å•†æ¥­æ¨¡å¼', 'å‹Ÿè³‡', 'å‰µæ–°å‰µæ¥­'],
                'tags': ['å‰µæ¥­', 'startup', 'å•†æ¥­'],
                'search_queries': [
                    'startup trends 2024',
                    'business innovation',
                    'entrepreneurship tips',
                    'startup funding'
                ]
            },
            'tech-trends': {
                'name': 'æŠ€è¡“è¶¨å‹¢',
                'keywords': ['ç§‘æŠ€', 'æŠ€è¡“', 'è¶¨å‹¢', 'Web3', 'å€å¡Šéˆ'],
                'tags': ['ç§‘æŠ€', 'æŠ€è¡“', 'è¶¨å‹¢'],
                'search_queries': [
                    'tech trends 2024',
                    'emerging technologies',
                    'future technology',
                    'tech innovations'
                ]
            },
            'personal-branding': {
                'name': 'å€‹äººå“ç‰Œ',
                'keywords': ['å€‹äººå“ç‰Œ', 'è‡ªåª’é«”', 'å…§å®¹å‰µä½œ', 'LinkedIn'],
                'tags': ['å€‹äººå“ç‰Œ', 'å…§å®¹å‰µä½œ', 'è‡ªåª’é«”'],
                'search_queries': [
                    'personal branding 2024',
                    'content creation tips',
                    'social media strategy',
                    'building online presence'
                ]
            }
        }

    def search_trending_topics(self, category_key: str, days_back: int = 7) -> List[Dict]:
        """
        æœå°‹è©²åˆ†é¡çš„ç†±é–€è©±é¡Œ
        ä½¿ç”¨å¤šå€‹ä¾†æºï¼šGoogle Trends APIã€News APIã€Redditã€Hacker News
        """
        print(f"ğŸ” æœå°‹ {self.categories[category_key]['name']} çš„ç†±é–€è©±é¡Œ...")

        category = self.categories[category_key]
        trending_topics = []

        # æ–¹æ³• 1: ä½¿ç”¨ News APIï¼ˆå…è²»ç‰ˆï¼Œéœ€è¦ API keyï¼‰
        trending_topics.extend(self._search_news_api(category))

        # æ–¹æ³• 2: ä½¿ç”¨ Hacker News APIï¼ˆå…è²»ï¼Œç„¡éœ€ keyï¼‰
        trending_topics.extend(self._search_hackernews(category))

        # æ–¹æ³• 3: ä½¿ç”¨ Reddit APIï¼ˆå…è²»ï¼Œç„¡éœ€ keyï¼‰
        trending_topics.extend(self._search_reddit(category))

        # æ–¹æ³• 4: ä½¿ç”¨ Google Trendsï¼ˆé€é serpapi æˆ– pytrendsï¼‰
        # trending_topics.extend(self._search_google_trends(category))

        # å»é‡ä¸¦æ’åºï¼ˆæŒ‰ç›¸é—œåº¦å’Œç†±åº¦ï¼‰
        unique_topics = self._deduplicate_and_rank(trending_topics)

        return unique_topics[:5]  # è¿”å›å‰ 5 å€‹æœ€ç†±é–€çš„è©±é¡Œ

    def _search_news_api(self, category: Dict) -> List[Dict]:
        """ä½¿ç”¨ News API æœå°‹æ–°è"""
        topics = []

        # News API éœ€è¦ API keyï¼Œå¦‚æœæ²’æœ‰å°±è·³é
        news_api_key = os.environ.get('NEWS_API_KEY')
        if not news_api_key:
            print("  âš ï¸  æœªè¨­å®š NEWS_API_KEYï¼Œè·³é News API")
            return topics

        try:
            for query in category['search_queries'][:2]:  # åªæœå°‹å‰ 2 å€‹æŸ¥è©¢
                url = f"https://newsapi.org/v2/everything"
                params = {
                    'q': query,
                    'language': 'en',
                    'sortBy': 'popularity',
                    'pageSize': 5,
                    'apiKey': news_api_key,
                    'from': (datetime.now() - timedelta(days=7)).isoformat()
                }

                response = requests.get(url, params=params, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    for article in data.get('articles', [])[:3]:
                        topics.append({
                            'title': article['title'],
                            'description': article.get('description', ''),
                            'url': article['url'],
                            'source': 'NewsAPI',
                            'published_at': article.get('publishedAt', ''),
                            'relevance': 0.8
                        })
        except Exception as e:
            print(f"  âš ï¸  News API æœå°‹å¤±æ•—: {e}")

        return topics

    def _search_hackernews(self, category: Dict) -> List[Dict]:
        """æœå°‹ Hacker News ç†±é–€æ–‡ç« """
        topics = []

        try:
            # ç²å–ç†±é–€æ–‡ç« 
            url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(url, timeout=10)

            if response.status_code == 200:
                story_ids = response.json()[:30]  # å–å‰ 30 ç¯‡

                # ç²å–æ–‡ç« è©³æƒ…
                for story_id in story_ids[:10]:  # åªæª¢æŸ¥å‰ 10 ç¯‡
                    story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    story_response = requests.get(story_url, timeout=5)

                    if story_response.status_code == 200:
                        story = story_response.json()

                        # æª¢æŸ¥æ˜¯å¦èˆ‡åˆ†é¡é—œéµå­—ç›¸é—œ
                        title = story.get('title', '').lower()
                        if any(keyword.lower() in title for keyword in category['keywords']):
                            topics.append({
                                'title': story.get('title', ''),
                                'description': story.get('text', '')[:200],
                                'url': story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                                'source': 'HackerNews',
                                'published_at': datetime.fromtimestamp(story.get('time', 0)).isoformat(),
                                'relevance': 0.7,
                                'score': story.get('score', 0)
                            })
        except Exception as e:
            print(f"  âš ï¸  Hacker News æœå°‹å¤±æ•—: {e}")

        return topics

    def _search_reddit(self, category: Dict) -> List[Dict]:
        """æœå°‹ Reddit ç†±é–€æ–‡ç« """
        topics = []

        # ç›¸é—œçš„ subreddit
        subreddits = {
            'ai-tools': ['artificial', 'MachineLearning', 'OpenAI', 'ChatGPT'],
            'creativity': ['creativity', 'design', 'DesignThinking'],
            'productivity': ['productivity', 'gtd', 'productivity'],
            'entrepreneurship': ['Entrepreneur', 'startups', 'smallbusiness'],
            'tech-trends': ['technology', 'Futurology', 'tech'],
            'personal-branding': ['personalbranding', 'socialmedia', 'marketing']
        }

        try:
            for subreddit in subreddits.get(list(self.categories.keys())[0], ['all'])[:2]:
                url = f"https://www.reddit.com/r/{subreddit}/hot.json"
                headers = {'User-Agent': 'AutoContentBot/1.0'}

                response = requests.get(url, headers=headers, timeout=10)

                if response.status_code == 200:
                    data = response.json()

                    for post in data['data']['children'][:5]:
                        post_data = post['data']

                        # æª¢æŸ¥æ˜¯å¦èˆ‡åˆ†é¡é—œéµå­—ç›¸é—œ
                        title = post_data.get('title', '').lower()
                        if any(keyword.lower() in title for keyword in category['keywords']):
                            topics.append({
                                'title': post_data.get('title', ''),
                                'description': post_data.get('selftext', '')[:200],
                                'url': f"https://reddit.com{post_data.get('permalink', '')}",
                                'source': 'Reddit',
                                'published_at': datetime.fromtimestamp(post_data.get('created_utc', 0)).isoformat(),
                                'relevance': 0.6,
                                'score': post_data.get('score', 0)
                            })
        except Exception as e:
            print(f"  âš ï¸  Reddit æœå°‹å¤±æ•—: {e}")

        return topics

    def _deduplicate_and_rank(self, topics: List[Dict]) -> List[Dict]:
        """å»é‡ä¸¦æŒ‰ç›¸é—œåº¦æ’åº"""
        # ç°¡å–®å»é‡ï¼ˆåŸºæ–¼æ¨™é¡Œç›¸ä¼¼åº¦ï¼‰
        unique_topics = []
        seen_titles = set()

        for topic in topics:
            title_lower = topic['title'].lower()
            # ç°¡å–®çš„å»é‡é‚è¼¯
            if not any(title_lower in seen or seen in title_lower for seen in seen_titles):
                unique_topics.append(topic)
                seen_titles.add(title_lower)

        # æ’åºï¼ˆæŒ‰ç›¸é—œåº¦å’Œåˆ†æ•¸ï¼‰
        unique_topics.sort(key=lambda x: (x.get('relevance', 0), x.get('score', 0)), reverse=True)

        return unique_topics

    def generate_article_with_ai(self, topic: Dict, category_key: str) -> str:
        """ä½¿ç”¨ AI ç”Ÿæˆæ–‡ç« """
        if not self.client:
            print("âŒ æœªè¨­å®š ANTHROPIC_API_KEYï¼Œç„¡æ³•ä½¿ç”¨ AI ç”Ÿæˆ")
            return None

        category = self.categories[category_key]

        print(f"ğŸ¤– ä½¿ç”¨ AI ç”Ÿæˆæ–‡ç« ï¼š{topic['title'][:50]}...")

        # æ§‹å»ºæç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ç§‘æŠ€éƒ¨è½æ ¼ä½œå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šæ’°å¯«ä¸€ç¯‡æ·±åº¦æ–‡ç« ï¼š

ä¸»é¡Œï¼š{topic['title']}
æè¿°ï¼š{topic.get('description', 'ç„¡')}
ä¾†æºé€£çµï¼š{topic.get('url', 'ç„¡')}

æ–‡ç« è¦æ±‚ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡
2. å­—æ•¸ï¼š2000-3000 å­—
3. é¢¨æ ¼ï¼šå°ˆæ¥­ä½†æ˜“æ‡‚ï¼Œæœ‰æ·±åº¦ä½†ä¸è‰±æ¾€
4. çµæ§‹ï¼š
   - å¼•è¨€ï¼ˆ100-200å­—ï¼‰ï¼šå¸å¼•è®€è€…ï¼Œèªªæ˜ç‚ºä»€éº¼é€™å€‹ä¸»é¡Œé‡è¦
   - æ ¸å¿ƒå…§å®¹ï¼ˆ1500-2000å­—ï¼‰ï¼šæ·±å…¥åˆ†æï¼ŒåŒ…å«å¯¦ä¾‹å’Œæ•¸æ“š
   - å¯¦éš›æ‡‰ç”¨ï¼ˆ300-500å­—ï¼‰ï¼šå¦‚ä½•æ‡‰ç”¨åˆ°å¯¦éš›å·¥ä½œä¸­
   - ç¸½çµèˆ‡å±•æœ›ï¼ˆ200-300å­—ï¼‰ï¼šé—œéµè¦é»å’Œæœªä¾†è¶¨å‹¢
5. åŒ…å«ï¼š
   - å…·é«”çš„ä¾‹å­å’Œæ¡ˆä¾‹
   - å¯¦ç”¨çš„å»ºè­°
   - æ¸…æ™°çš„æ¨™é¡Œçµæ§‹ï¼ˆä½¿ç”¨ ##ã€### æ¨™è¨˜ï¼‰
   - é©ç•¶ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ä¾†çµ„ç¹”è³‡è¨Š
   - ä½¿ç”¨ emoji å¢åŠ å¯è®€æ€§ï¼ˆä½†ä¸è¦éåº¦ï¼‰
6. èªèª¿ï¼šé¼“å‹µæ€§ã€å¯¦ç”¨æ€§ã€å‰ç»æ€§
7. é¿å…ï¼šéåº¦å®£å‚³ã€ä¸å¯¦è³‡è¨Šã€æŠ„è¥²

è«‹ç›´æ¥è¼¸å‡ºå®Œæ•´çš„ Markdown æ ¼å¼æ–‡ç« å…§å®¹ï¼ˆä¸åŒ…å« front matterï¼‰ï¼Œå¾å¼•è¨€é–‹å§‹ã€‚
"""

        try:
            message = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=4000,
                temperature=0.7,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            article_content = message.content[0].text

            # ç”Ÿæˆ front matter
            title = self._generate_title_from_content(article_content, topic['title'])
            excerpt = self._generate_excerpt_from_content(article_content)

            front_matter = f"""---
layout: single
title: "{title}"
date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} +0800
categories:
{self._format_yaml_list([category['name']])}
tags:
{self._format_yaml_list(category['tags'])}
excerpt: "{excerpt}"
---

"""

            full_article = front_matter + article_content

            # æ·»åŠ ç›¸é—œé€£çµ
            full_article += f"\n\n---\n\n**åƒè€ƒè³‡æ–™ï¼š**\n- [{topic['title']}]({topic['url']})\n"

            return full_article

        except Exception as e:
            print(f"âŒ AI ç”Ÿæˆæ–‡ç« å¤±æ•—: {e}")
            return None

    def _generate_title_from_content(self, content: str, fallback: str) -> str:
        """å¾å…§å®¹ä¸­æå–æˆ–ç”Ÿæˆæ¨™é¡Œ"""
        # å˜—è©¦å¾å…§å®¹ä¸­æ‰¾åˆ°ç¬¬ä¸€å€‹æ¨™é¡Œ
        lines = content.split('\n')
        for line in lines:
            if line.startswith('# '):
                return line.replace('# ', '').strip()

        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ fallback æˆ–ç”Ÿæˆä¸€å€‹
        return fallback[:100]

    def _generate_excerpt_from_content(self, content: str) -> str:
        """å¾å…§å®¹ç”Ÿæˆæ‘˜è¦"""
        # å–ç¬¬ä¸€æ®µéæ¨™é¡Œçš„æ–‡å­—ä½œç‚ºæ‘˜è¦
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and len(line) > 50:
                # å–å‰ 150 å­—
                excerpt = line[:150]
                if len(line) > 150:
                    excerpt += '...'
                return excerpt

        return "æ¢ç´¢æœ€æ–°çš„æŠ€è¡“è¶¨å‹¢å’Œå¯¦ç”¨è¦‹è§£"

    def _format_yaml_list(self, items: List[str]) -> str:
        """æ ¼å¼åŒ– YAML åˆ—è¡¨"""
        return '\n'.join([f"  - {item}" for item in items])

    def save_article(self, content: str, category_key: str) -> str:
        """ä¿å­˜æ–‡ç« åˆ° _posts ç›®éŒ„"""
        posts_dir = Path('_posts')
        posts_dir.mkdir(exist_ok=True)

        # ç”Ÿæˆæª”æ¡ˆå
        today = datetime.now().strftime('%Y-%m-%d')
        # å¾å…§å®¹æå–æ¨™é¡Œä½œç‚ºæª”åçš„ä¸€éƒ¨åˆ†
        title_for_filename = self._extract_title_for_filename(content)
        filename = f"{today}-{category_key}-{title_for_filename}.md"
        filepath = posts_dir / filename

        # ç¢ºä¿æª”åå”¯ä¸€
        counter = 1
        while filepath.exists():
            filename = f"{today}-{category_key}-{title_for_filename}-{counter}.md"
            filepath = posts_dir / filename
            counter += 1

        # ä¿å­˜æ–‡ç« 
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"âœ… æ–‡ç« å·²ä¿å­˜ï¼š{filepath}")
        return str(filepath)

    def _extract_title_for_filename(self, content: str) -> str:
        """å¾æ–‡ç« å…§å®¹æå–æ¨™é¡Œç”¨æ–¼æª”å"""
        # å¾ front matter æå– title
        import re
        match = re.search(r'title:\s*"(.+?)"', content)
        if match:
            title = match.group(1)
            # è½‰æ›ç‚ºé©åˆæª”åçš„æ ¼å¼
            title = re.sub(r'[^\w\s-]', '', title.lower())
            title = re.sub(r'[-\s]+', '-', title)
            return title[:50]  # é™åˆ¶é•·åº¦

        return 'article'

    def run(self, categories: List[str], article_count: int = 1) -> List[str]:
        """é‹è¡Œè‡ªå‹•å…§å®¹ç”Ÿæˆ"""
        print(f"\n{'='*60}")
        print("ğŸš€ è‡ªå‹•å…§å®¹ç”Ÿæˆå™¨")
        print(f"{'='*60}\n")

        generated_files = []

        for category_key in categories:
            if category_key not in self.categories:
                print(f"âš ï¸  æœªçŸ¥åˆ†é¡ï¼š{category_key}ï¼Œè·³é")
                continue

            print(f"\nğŸ“‚ è™•ç†åˆ†é¡ï¼š{self.categories[category_key]['name']}")
            print("-" * 60)

            # 1. æœå°‹ç†±é–€è©±é¡Œ
            topics = self.search_trending_topics(category_key)

            if not topics:
                print(f"  âš ï¸  æœªæ‰¾åˆ°ç›¸é—œè©±é¡Œï¼Œè·³éæ­¤åˆ†é¡")
                continue

            print(f"  âœ… æ‰¾åˆ° {len(topics)} å€‹ç†±é–€è©±é¡Œ")

            # 2. ç‚ºæ¯å€‹è©±é¡Œç”Ÿæˆæ–‡ç« ï¼ˆæœ€å¤š article_count ç¯‡ï¼‰
            for i, topic in enumerate(topics[:article_count], 1):
                print(f"\n  ğŸ“ [{i}/{min(len(topics), article_count)}] ç”Ÿæˆæ–‡ç« ...")
                print(f"     è©±é¡Œï¼š{topic['title']}")
                print(f"     ä¾†æºï¼š{topic['source']}")

                # ç”Ÿæˆæ–‡ç« 
                article = self.generate_article_with_ai(topic, category_key)

                if article:
                    # ä¿å­˜æ–‡ç« 
                    filepath = self.save_article(article, category_key)
                    generated_files.append(filepath)
                else:
                    print(f"  âŒ æ–‡ç« ç”Ÿæˆå¤±æ•—")

        return generated_files


def main():
    parser = argparse.ArgumentParser(description='è‡ªå‹•å…§å®¹ç”Ÿæˆå™¨')
    parser.add_argument(
        '--categories',
        nargs='+',
        choices=['all', 'ai-tools', 'creativity', 'productivity',
                 'entrepreneurship', 'tech-trends', 'personal-branding'],
        default=['all'],
        help='è¦ç”Ÿæˆå…§å®¹çš„åˆ†é¡'
    )
    parser.add_argument(
        '--count',
        type=int,
        default=1,
        help='æ¯å€‹åˆ†é¡ç”Ÿæˆå¹¾ç¯‡æ–‡ç« '
    )
    parser.add_argument(
        '--api-key',
        help='Anthropic API Keyï¼ˆæˆ–ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ ANTHROPIC_API_KEYï¼‰'
    )

    args = parser.parse_args()

    # è™•ç† 'all' é¸é …
    if 'all' in args.categories:
        categories = ['ai-tools', 'creativity', 'productivity',
                      'entrepreneurship', 'tech-trends', 'personal-branding']
    else:
        categories = args.categories

    # å‰µå»ºç”Ÿæˆå™¨
    generator = AutoContentGenerator(api_key=args.api_key)

    # é‹è¡Œç”Ÿæˆ
    generated_files = generator.run(categories, args.count)

    # ç¸½çµ
    print(f"\n{'='*60}")
    print("ğŸ“Š ç”Ÿæˆç¸½çµ")
    print(f"{'='*60}")
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_files)} ç¯‡æ–‡ç« ")
    print(f"\nç”Ÿæˆçš„æ–‡ç« ï¼š")
    for filepath in generated_files:
        print(f"  - {filepath}")

    if generated_files:
        print(f"\nä¸‹ä¸€æ­¥ï¼š")
        print(f"1. æª¢æŸ¥ç”Ÿæˆçš„æ–‡ç« ï¼šls -la _posts/")
        print(f"2. æäº¤åˆ° Gitï¼š")
        print(f"   git add _posts/")
        print(f"   git commit -m 'ğŸ¤– è‡ªå‹•ç”Ÿæˆ {len(generated_files)} ç¯‡æ–‡ç« '")
        print(f"   git push")
        print(f"3. GitHub Actions æœƒè‡ªå‹•éƒ¨ç½²åˆ° GitHub Pages")


if __name__ == '__main__':
    main()
