#!/usr/bin/env python3
"""
é€£çµæ¸¬è©¦è…³æœ¬
æª¢æŸ¥ç¶²ç«™ä¸­çš„æ‰€æœ‰å…§éƒ¨å’Œå¤–éƒ¨é€£çµæ˜¯å¦æœ‰æ•ˆ
"""

import os
import sys
import json
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import requests
from collections import defaultdict

class LinkTester:
    def __init__(self, site_dir="_site"):
        self.site_dir = Path(site_dir)
        self.base_url = "/"
        self.internal_links = set()
        self.external_links = set()
        self.broken_links = []
        self.warnings = []

    def find_all_html_files(self):
        """æ‰¾åˆ°æ‰€æœ‰ HTML æ–‡ä»¶"""
        return list(self.site_dir.rglob("*.html"))

    def extract_links(self, html_file):
        """å¾ HTML æ–‡ä»¶æå–æ‰€æœ‰é€£çµ"""
        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')

            links = []

            # æå– <a> æ¨™ç±¤çš„ href
            for tag in soup.find_all('a', href=True):
                href = tag['href']
                if href:
                    links.append(('a', href, tag.get_text(strip=True)[:50]))

            # æå– <img> æ¨™ç±¤çš„ src
            for tag in soup.find_all('img', src=True):
                src = tag['src']
                if src:
                    links.append(('img', src, tag.get('alt', '')[:50]))

            # æå– <link> æ¨™ç±¤çš„ hrefï¼ˆCSS ç­‰ï¼‰
            for tag in soup.find_all('link', href=True):
                href = tag['href']
                if href:
                    links.append(('link', href, tag.get('rel', [''])[0]))

            # æå– <script> æ¨™ç±¤çš„ src
            for tag in soup.find_all('script', src=True):
                src = tag['src']
                if src:
                    links.append(('script', src, ''))

            return links

        except Exception as e:
            self.warnings.append(f"è§£æ {html_file} æ™‚å‡ºéŒ¯: {e}")
            return []

    def is_internal_link(self, url):
        """åˆ¤æ–·æ˜¯å¦ç‚ºå…§éƒ¨é€£çµ"""
        parsed = urlparse(url)
        return not parsed.netloc or parsed.netloc in ['localhost', '127.0.0.1']

    def normalize_path(self, url, current_file):
        """æ¨™æº–åŒ–è·¯å¾‘"""
        # ç§»é™¤éŒ¨é»
        url = url.split('#')[0]

        # è·³éç‰¹æ®Šå”è­°
        if url.startswith(('mailto:', 'tel:', 'javascript:')):
            return None

        # è™•ç†çµ•å°è·¯å¾‘
        if url.startswith('/'):
            return self.site_dir / url.lstrip('/')

        # è™•ç†ç›¸å°è·¯å¾‘
        if not url.startswith(('http://', 'https://')):
            current_dir = current_file.parent
            relative_path = (current_dir / url).resolve()
            return relative_path

        return url

    def check_internal_link(self, link_path):
        """æª¢æŸ¥å…§éƒ¨é€£çµæ˜¯å¦å­˜åœ¨"""
        if isinstance(link_path, str):
            # å¤–éƒ¨é€£çµï¼Œç¨å¾Œè™•ç†
            return True

        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if link_path.exists():
            return True

        # å˜—è©¦æ·»åŠ  .html
        if link_path.with_suffix('.html').exists():
            return True

        # å˜—è©¦ index.html
        if link_path.is_dir():
            index_path = link_path / 'index.html'
            if index_path.exists():
                return True

        return False

    def check_external_link(self, url, timeout=10):
        """æª¢æŸ¥å¤–éƒ¨é€£çµæ˜¯å¦å¯è¨ªå•"""
        try:
            response = requests.head(url, timeout=timeout, allow_redirects=True)
            return response.status_code < 400
        except requests.RequestException:
            # å¦‚æœ HEAD å¤±æ•—ï¼Œå˜—è©¦ GET
            try:
                response = requests.get(url, timeout=timeout, allow_redirects=True, stream=True)
                return response.status_code < 400
            except:
                return False

    def run_tests(self, check_external=False):
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("ğŸ” é–‹å§‹æª¢æŸ¥é€£çµ...")

        html_files = self.find_all_html_files()
        print(f"ğŸ“„ æ‰¾åˆ° {len(html_files)} å€‹ HTML æ–‡ä»¶")

        # æ”¶é›†æ‰€æœ‰é€£çµ
        all_links = defaultdict(list)

        for html_file in html_files:
            links = self.extract_links(html_file)
            relative_path = html_file.relative_to(self.site_dir)

            for link_type, url, text in links:
                normalized = self.normalize_path(url, html_file)

                if normalized is None:
                    continue

                if isinstance(normalized, str):
                    # å¤–éƒ¨é€£çµ
                    self.external_links.add(normalized)
                    all_links[normalized].append({
                        'file': str(relative_path),
                        'type': link_type,
                        'text': text
                    })
                else:
                    # å…§éƒ¨é€£çµ
                    self.internal_links.add(normalized)
                    all_links[normalized].append({
                        'file': str(relative_path),
                        'type': link_type,
                        'text': text
                    })

        # æª¢æŸ¥å…§éƒ¨é€£çµ
        print(f"\nğŸ”— æª¢æŸ¥ {len(self.internal_links)} å€‹å…§éƒ¨é€£çµ...")
        for link in self.internal_links:
            if not self.check_internal_link(link):
                sources = all_links[link]
                self.broken_links.append({
                    'type': 'internal',
                    'url': str(link),
                    'sources': sources
                })

        # æª¢æŸ¥å¤–éƒ¨é€£çµï¼ˆå¯é¸ï¼‰
        if check_external:
            print(f"\nğŸŒ æª¢æŸ¥ {len(self.external_links)} å€‹å¤–éƒ¨é€£çµ...")
            for i, link in enumerate(self.external_links, 1):
                print(f"  [{i}/{len(self.external_links)}] {link[:60]}...")
                if not self.check_external_link(link):
                    sources = all_links[link]
                    self.broken_links.append({
                        'type': 'external',
                        'url': link,
                        'sources': sources
                    })

        # ç”Ÿæˆå ±å‘Š
        self.generate_report()

    def generate_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š é€£çµæ¸¬è©¦å ±å‘Š")
        print("="*60)

        print(f"\nâœ… ç¸½è¨ˆ:")
        print(f"  - å…§éƒ¨é€£çµ: {len(self.internal_links)}")
        print(f"  - å¤–éƒ¨é€£çµ: {len(self.external_links)}")
        print(f"  - æå£é€£çµ: {len(self.broken_links)}")

        if self.warnings:
            print(f"\nâš ï¸  è­¦å‘Š ({len(self.warnings)}):")
            for warning in self.warnings:
                print(f"  - {warning}")

        if self.broken_links:
            print(f"\nâŒ æå£çš„é€£çµ ({len(self.broken_links)}):")
            for i, broken in enumerate(self.broken_links, 1):
                print(f"\n  {i}. [{broken['type']}] {broken['url']}")
                print(f"     å‡ºç¾åœ¨ä»¥ä¸‹æ–‡ä»¶:")
                for source in broken['sources'][:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
                    print(f"     - {source['file']} ({source['type']}): {source['text']}")
                if len(broken['sources']) > 5:
                    print(f"     ... é‚„æœ‰ {len(broken['sources']) - 5} å€‹")

            # ä¿å­˜è©³ç´°å ±å‘Š
            report_dir = Path('_tests/reports')
            report_dir.mkdir(parents=True, exist_ok=True)

            report_file = report_dir / 'broken_links.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'total_internal': len(self.internal_links),
                    'total_external': len(self.external_links),
                    'broken_count': len(self.broken_links),
                    'broken_links': self.broken_links,
                    'warnings': self.warnings
                }, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")

            # å¤±æ•—é€€å‡º
            sys.exit(1)
        else:
            print("\nğŸ‰ æ‰€æœ‰é€£çµéƒ½æ­£å¸¸ï¼")
            sys.exit(0)

def main():
    import argparse

    parser = argparse.ArgumentParser(description='æª¢æŸ¥ç¶²ç«™é€£çµ')
    parser.add_argument('--site-dir', default='_site', help='ç¶²ç«™ç›®éŒ„')
    parser.add_argument('--check-external', action='store_true', help='æª¢æŸ¥å¤–éƒ¨é€£çµ')

    args = parser.parse_args()

    tester = LinkTester(args.site_dir)
    tester.run_tests(check_external=args.check_external)

if __name__ == '__main__':
    main()
